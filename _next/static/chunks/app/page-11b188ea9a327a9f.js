(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{8774:(e,a,l)=>{Promise.resolve().then(l.bind(l,9986))},9986:(e,a,l)=>{"use strict";l.d(a,{default:()=>b});var t=l(5155),r=l(2115);let s=e=>{let a=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,a,l)=>l?l.toUpperCase():a.toLowerCase());return a.charAt(0).toUpperCase()+a.slice(1)},c=function(){for(var e=arguments.length,a=Array(e),l=0;l<e;l++)a[l]=arguments[l];return a.filter((e,a,l)=>!!e&&""!==e.trim()&&l.indexOf(e)===a).join(" ").trim()};var i={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let n=(0,r.forwardRef)((e,a)=>{let{color:l="currentColor",size:t=24,strokeWidth:s=2,absoluteStrokeWidth:n,className:m="",children:o,iconNode:d,...h}=e;return(0,r.createElement)("svg",{ref:a,...i,width:t,height:t,stroke:l,strokeWidth:n?24*Number(s)/Number(t):s,className:c("lucide",m),...!o&&!(e=>{for(let a in e)if(a.startsWith("aria-")||"role"===a||"title"===a)return!0})(h)&&{"aria-hidden":"true"},...h},[...d.map(e=>{let[a,l]=e;return(0,r.createElement)(a,l)}),...Array.isArray(o)?o:[o]])}),m=(e,a)=>{let l=(0,r.forwardRef)((l,t)=>{let{className:i,...m}=l;return(0,r.createElement)(n,{ref:t,iconNode:a,className:c("lucide-".concat(s(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()),"lucide-".concat(e),i),...m})});return l.displayName=s(e),l},o=m("search",[["path",{d:"m21 21-4.34-4.34",key:"14j7rj"}],["circle",{cx:"11",cy:"11",r:"8",key:"4ej97u"}]]),d=m("rotate-ccw",[["path",{d:"M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8",key:"1357e3"}],["path",{d:"M3 3v5h5",key:"1xhq8a"}]]),h=[{name:"Gemma3-1B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.298,stderr:.0126},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.2024,stderr:.0111}]},{name:"Gemma3-1B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.2768,stderr:.0031},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc_norm",value:.2768,stderr:.0031},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.2617,stderr:.0074},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.2621,stderr:.0105},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.2792,stderr:.0047},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.2865,stderr:.0058}]},{name:"Gemma3-4B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.6126,stderr:.0134},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.4602,stderr:.0137}]},{name:"Gemma3-4B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.3911,stderr:.0034},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.3863,stderr:.0081},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.3205,stderr:.011},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3767,stderr:.0051},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.4366,stderr:.0063}]},{name:"Gemma3-12B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.8271,stderr:.0104},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.7968,stderr:.0111}]},{name:"Gemma3-27B-IT-QAT-Q4_0 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.859,stderr:.0096},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.8514,stderr:.0098}]},{name:"Gemma3n:E2B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.6907,stderr:.0127},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.602,stderr:.0135}]},{name:"Gemma3n:E4B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.7726,stderr:.0115},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.6763,stderr:.0129}]},{name:"GPT-OSS-20B-MXFP4 (llama.cpp)",description:"HF: bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental, ThinkLevel: medium - 運行約 1 小時半",category:"gpt-oss",hardware:"RTX4090",framework:"llama.cpp",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.4344,stderr:.0137},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.3397,stderr:.013}]},{name:"GPT-OSS-20B-MXFP4 (ollama)",description:"ollama 官方儲存庫, MXFP4, ThinkLevel: medium - 運行約 1 小時",category:"gpt-oss",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.395,stderr:.0135},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.1759,stderr:.0105}]},{name:"Llama-3-Taiwan-8B-Instruct-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"llama3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.7195,stderr:.0124},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.7187,stderr:.0124}]},{name:"Llama-3.1-TAIDE-LX-8B-Chat-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"llama3.1",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.1941,stderr:.0109},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:0,stderr:0},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.3866,stderr:.0034},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.354,stderr:.008},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.354,stderr:.0114},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3978,stderr:.0051},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.4047,stderr:.0063}]}],u=e=>(100*e).toFixed(2)+"%",g=m("trophy",[["path",{d:"M10 14.66v1.626a2 2 0 0 1-.976 1.696A5 5 0 0 0 7 21.978",key:"1n3hpd"}],["path",{d:"M14 14.66v1.626a2 2 0 0 0 .976 1.696A5 5 0 0 1 17 21.978",key:"rfe1zi"}],["path",{d:"M18 9h1.5a1 1 0 0 0 0-5H18",key:"7xy6bh"}],["path",{d:"M4 22h16",key:"57wxv0"}],["path",{d:"M6 9a6 6 0 0 0 12 0V3a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z",key:"1mhfuq"}],["path",{d:"M6 9H4.5a1 1 0 0 1 0-5H6",key:"tex48p"}]]),v=m("medal",[["path",{d:"M7.21 15 2.66 7.14a2 2 0 0 1 .13-2.2L4.4 2.8A2 2 0 0 1 6 2h12a2 2 0 0 1 1.6.8l1.6 2.14a2 2 0 0 1 .14 2.2L16.79 15",key:"143lza"}],["path",{d:"M11 12 5.12 2.2",key:"qhuxz6"}],["path",{d:"m13 12 5.88-9.8",key:"hbye0f"}],["path",{d:"M8 7h8",key:"i86dvs"}],["circle",{cx:"12",cy:"17",r:"5",key:"qbz8iq"}],["path",{d:"M12 18v-2h-.5",key:"fawc4q"}]]),x=m("award",[["path",{d:"m15.477 12.89 1.515 8.526a.5.5 0 0 1-.81.47l-3.58-2.687a1 1 0 0 0-1.197 0l-3.586 2.686a.5.5 0 0 1-.81-.469l1.514-8.526",key:"1yiouv"}],["circle",{cx:"12",cy:"8",r:"6",key:"1vp47v"}]]),p=(0,r.memo)(e=>{let{rank:a}=e;return(0,t.jsx)("div",{className:"rank-cell ".concat(a<=3?"top-rank":""),children:(0,t.jsxs)("div",{className:"flex items-center justify-center gap-1",children:[1===a?(0,t.jsx)(g,{className:"w-4 h-4 text-yellow-500"}):2===a?(0,t.jsx)(v,{className:"w-4 h-4 text-gray-400"}):3===a?(0,t.jsx)(x,{className:"w-4 h-4 text-amber-600"}):null,(0,t.jsx)("span",{children:a})]})})});p.displayName="RankCell";let k=(0,r.memo)(e=>{let{name:a,description:l,category:r,hardware:s}=e;return(0,t.jsxs)("div",{className:"model-info",children:[(0,t.jsx)("div",{className:"model-icon",style:{backgroundColor:{gemma3:"#3b82f6",gemma3n:"#8b5cf6","gpt-oss":"#10b981",llama3:"#f59e0b","llama3.1":"#ef4444"}[r]||"#6b7280"},children:a.charAt(0).toUpperCase()}),(0,t.jsxs)("div",{children:[(0,t.jsx)("div",{className:"model-name",children:a}),(0,t.jsx)("div",{className:"model-description",children:l})]})]})});k.displayName="ModelInfo";let f=(0,r.memo)(e=>{let{score:a}=e;return(0,t.jsx)("div",{className:"score-cell ".concat((e=>null==e?"score-na":e>=.7?"score-high":e>=.4?"score-medium":"score-low")(a)),children:null!=a?u(a):"—"})});f.displayName="ScoreCell";let j=(0,r.memo)(e=>{let{category:a}=e;return(0,t.jsx)("span",{className:"category-badge ".concat({gemma3:"badge-gemma3",gemma3n:"badge-gemma3n","gpt-oss":"badge-gpt-oss",llama3:"badge-llama3","llama3.1":"badge-llama31"}[a]||"badge-gemma3"),children:a})});function b(){let{data:e,stats:a,benchmark:l,category:s,framework:c,searchTerm:i,setBenchmark:n,setCategory:m,setFramework:g,setSearchTerm:v,resetFilters:x,isEmpty:b,hasFilters:w}=(()=>{let[e,a]=(0,r.useState)("gsm8k-flex"),[l,t]=(0,r.useState)("all"),[s,c]=(0,r.useState)("all"),[i,n]=(0,r.useState)(""),m=(0,r.useMemo)(()=>h.map(a=>{var l,t,r,s,c,i,n;let m=a.scores.find(e=>"gsm8k"===e.task&&"flexible-extract"===e.filter),o=a.scores.find(e=>"gsm8k"===e.task&&"strict-match"===e.filter),d=a.scores.find(e=>"tmmluplus"===e.task),h=a.scores.find(e=>"tmmluplus_STEM"===e.task),u=a.scores.find(e=>"tmmluplus_humanities"===e.task),g=a.scores.find(e=>"tmmluplus_other"===e.task),v=a.scores.find(e=>"tmmluplus_social_sciences"===e.task),x=null!=(l=null==m?void 0:m.value)?l:null,p=null!=(t=null==o?void 0:o.value)?t:null,k=null!=(r=null==d?void 0:d.value)?r:null,f=null!=(s=null==h?void 0:h.value)?s:null,j=null!=(c=null==u?void 0:u.value)?c:null,b=null!=(i=null==g?void 0:g.value)?i:null,w=null!=(n=null==v?void 0:v.value)?n:null,S=a.framework||"unknown",y=null;switch(e){case"gsm8k-flex":y=x;break;case"gsm8k-strict":y=p;break;case"tmmluplus":y=k;break;case"tmmluplus_STEM":y=f;break;case"tmmluplus_humanities":y=j;break;case"tmmluplus_other":y=b;break;case"tmmluplus_social_sciences":y=w}return{rank:0,name:a.name,description:a.description,category:a.category,score:y,allScores:{gsm8kFlex:x,gsm8kStrict:p,tmmluplus:k,tmmluplusSTEM:f,tmmluplusHumanities:j,tmmluplusOther:b,tmmluplus_social_sciences:w},hardware:a.hardware,framework:S}}).sort((e,a)=>null===e.score&&null!==a.score?1:null!==e.score&&null===a.score?-1:null===e.score&&null===a.score?0:a.score-e.score).map((e,a)=>({...e,rank:a+1})),[e]),o=(0,r.useMemo)(()=>m.filter(e=>{let a=e.name.toLowerCase().includes(i.toLowerCase()),t="all"===l||e.category===l,r="all"===s||e.framework===s;return a&&t&&r}),[m,i,l,s]),d=(0,r.useMemo)(()=>{let e=h.length,a=m.filter(e=>null!==e.score).length,l=m.filter(e=>null!==e.score).reduce((e,a)=>e+(a.score||0),0)/a,t=Math.max(...m.map(e=>e.score||0));return{totalModels:e,modelsWithCurrentScore:a,avgScore:isNaN(l)?0:l,topScore:isFinite(t)?t:0}},[m]),u=(0,r.useCallback)(e=>{a(e)},[]),g=(0,r.useCallback)(e=>{t(e)},[]),v=(0,r.useCallback)(e=>{n(e)},[]),x=(0,r.useCallback)(()=>{t("all"),c("all"),n("")},[]);return{data:o,stats:d,benchmark:e,category:l,framework:s,searchTerm:i,setBenchmark:u,setCategory:g,setFramework:(0,r.useCallback)(e=>{c(e)},[]),setSearchTerm:v,resetFilters:x,isEmpty:0===o.length,hasFilters:"all"!==l||"all"!==s||""!==i}})();return(0,t.jsxs)("div",{className:"arena-container",children:[(0,t.jsxs)("header",{className:"arena-header",children:[(0,t.jsx)("h1",{className:"arena-title",children:"LLM Benchmark Arena"}),(0,t.jsx)("p",{className:"arena-subtitle",children:"View rankings across various LLMs on their mathematical reasoning and Traditional Chinese understanding capabilities."}),(0,t.jsxs)("div",{className:"arena-stats",children:[(0,t.jsx)("span",{children:"Last Updated: Aug 16, 2025"}),(0,t.jsxs)("span",{children:["Total Models: ",a.totalModels]}),(0,t.jsxs)("span",{children:["Active Models: ",a.modelsWithCurrentScore]}),(0,t.jsxs)("span",{children:["Top Score: ",u(a.topScore)]}),(0,t.jsxs)("span",{children:["Average: ",u(a.avgScore)]})]})]}),(0,t.jsx)("div",{className:"arena-controls",children:(0,t.jsxs)("div",{className:"control-group",children:[(0,t.jsx)("div",{className:"select-wrapper",children:(0,t.jsx)("select",{value:l,onChange:e=>n(e.target.value),className:"arena-select",children:[{value:"gsm8k-flex",label:"GSM8K (Flexible)",emoji:"\uD83D\uDCCA"},{value:"gsm8k-strict",label:"GSM8K (Strict)",emoji:"\uD83D\uDCCA"},{value:"tmmluplus",label:"TMMLU+",emoji:"\uD83C\uDDF9\uD83C\uDDFC"},{value:"tmmluplus_STEM",label:"TMMLU+ STEM",emoji:"\uD83D\uDD2C"},{value:"tmmluplus_humanities",label:"TMMLU+ Humanities",emoji:"\uD83D\uDCDA"},{value:"tmmluplus_other",label:"TMMLU+ Other",emoji:"\uD83D\uDCDD"},{value:"tmmluplus_social_sciences",label:"TMMLU+ Social Sciences",emoji:"\uD83C\uDFDB️"}].map(e=>(0,t.jsxs)("option",{value:e.value,children:[e.emoji," ",e.label]},e.value))})}),(0,t.jsx)("div",{className:"select-wrapper",children:(0,t.jsx)("select",{value:s,onChange:e=>m(e.target.value),className:"arena-select",children:[{value:"all",label:"All Categories"},{value:"gemma3",label:"Gemma3"},{value:"gemma3n",label:"Gemma3n"},{value:"gpt-oss",label:"GPT-OSS"},{value:"llama3",label:"Llama3"},{value:"llama3.1",label:"Llama3.1"}].map(e=>(0,t.jsx)("option",{value:e.value,children:e.label},e.value))})}),(0,t.jsx)("div",{className:"select-wrapper",children:(0,t.jsx)("select",{value:c,onChange:e=>g(e.target.value),className:"arena-select",children:[{value:"all",label:"All Frameworks",emoji:"\uD83D\uDD27"},{value:"huggingface",label:"Hugging Face",emoji:"\uD83E\uDD17"},{value:"ollama",label:"Ollama",emoji:"\uD83E\uDD99"},{value:"llama.cpp",label:"llama.cpp",emoji:"⚡"}].map(e=>(0,t.jsxs)("option",{value:e.value,children:[e.emoji," ",e.label]},e.value))})}),(0,t.jsxs)("div",{className:"search-container",children:[(0,t.jsx)(o,{className:"search-icon"}),(0,t.jsx)("input",{type:"text",placeholder:"Search by model name...",value:i,onChange:e=>v(e.target.value),className:"search-input"})]}),w&&(0,t.jsxs)("button",{onClick:x,className:"reset-button",title:"Reset filters",children:[(0,t.jsx)(d,{className:"w-4 h-4"}),"Reset"]})]})}),w&&(0,t.jsxs)("div",{className:"px-4 py-2 text-sm text-slate-400",children:["Showing ",e.length," of ",a.totalModels," models",i&&' matching "'.concat(i,'"'),"all"!==s&&" in ".concat(s)]}),(0,t.jsx)("div",{className:"arena-table-container",children:b?(0,t.jsxs)("div",{className:"p-8 text-center",children:[(0,t.jsx)("div",{className:"text-slate-500 mb-2",children:"No models found"}),(0,t.jsx)("div",{className:"text-sm text-slate-400",children:"Try adjusting your search criteria or filters"})]}):(0,t.jsxs)("table",{className:"arena-table",children:[(0,t.jsx)("thead",{children:(0,t.jsxs)("tr",{children:[(0,t.jsx)("th",{children:"Rank"}),(0,t.jsx)("th",{children:"Model"}),(0,t.jsx)("th",{children:"Score"}),(0,t.jsx)("th",{children:"Category"}),(0,t.jsx)("th",{children:"Hardware"})]})}),(0,t.jsx)("tbody",{children:e.map(e=>(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{children:(0,t.jsx)(p,{rank:e.rank})}),(0,t.jsx)("td",{children:(0,t.jsx)(k,{name:e.name,description:e.description,category:e.category,hardware:e.hardware})}),(0,t.jsx)("td",{children:(0,t.jsx)(f,{score:e.score})}),(0,t.jsx)("td",{children:(0,t.jsx)(j,{category:e.category})}),(0,t.jsx)("td",{className:"text-center",children:e.hardware?(0,t.jsx)("span",{className:"text-xs bg-red-500/20 text-red-400 px-2 py-1 rounded",children:e.hardware}):(0,t.jsx)("span",{className:"text-gray-500",children:"RTX4090"})})]},e.name))})]})}),(0,t.jsx)("div",{className:"mt-8 p-4 bg-slate-800 rounded-lg border border-slate-600",children:(0,t.jsxs)("div",{className:"grid md:grid-cols-2 gap-4 text-sm text-slate-300",children:[(0,t.jsxs)("div",{children:[(0,t.jsx)("h4",{className:"font-semibold text-slate-200 mb-2",children:"Test Environment"}),(0,t.jsxs)("ul",{className:"space-y-1",children:[(0,t.jsx)("li",{children:"• OS: Linux (Ubuntu 22.04)"}),(0,t.jsx)("li",{children:"• Primary GPU: RTX4090 24GB"}),(0,t.jsx)("li",{children:"• Secondary GPU: RTX5090 32GB (marked)"}),(0,t.jsx)("li",{children:"• Framework: EleutherAI/lm-evaluation-harness"})]})]}),(0,t.jsxs)("div",{children:[(0,t.jsx)("h4",{className:"font-semibold text-slate-200 mb-2",children:"Benchmarks"}),(0,t.jsxs)("ul",{className:"space-y-1",children:[(0,t.jsxs)("li",{children:["• ",(0,t.jsx)("strong",{children:"GSM8K:"})," Mathematical reasoning problems"]}),(0,t.jsxs)("li",{children:["• ",(0,t.jsx)("strong",{children:"TMMLU+:"})," Taiwan Traditional Chinese understanding"]}),(0,t.jsxs)("li",{children:["• ",(0,t.jsx)("strong",{children:"Flexible:"})," Lenient answer extraction"]}),(0,t.jsxs)("li",{children:["• ",(0,t.jsx)("strong",{children:"Strict:"})," Exact answer matching"]})]})]})]})})]})}j.displayName="CategoryBadge"}},e=>{e.O(0,[441,964,358],()=>e(e.s=8774)),_N_E=e.O()}]);