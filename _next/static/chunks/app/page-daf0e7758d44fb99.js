(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{8774:(e,a,t)=>{Promise.resolve().then(t.bind(t,9986))},9986:(e,a,t)=>{"use strict";t.d(a,{default:()=>S});var r=t(5155),l=t(2115);let s=e=>{let a=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,a,t)=>t?t.toUpperCase():a.toLowerCase());return a.charAt(0).toUpperCase()+a.slice(1)},c=function(){for(var e=arguments.length,a=Array(e),t=0;t<e;t++)a[t]=arguments[t];return a.filter((e,a,t)=>!!e&&""!==e.trim()&&t.indexOf(e)===a).join(" ").trim()};var i={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let n=(0,l.forwardRef)((e,a)=>{let{color:t="currentColor",size:r=24,strokeWidth:s=2,absoluteStrokeWidth:n,className:m="",children:o,iconNode:u,...h}=e;return(0,l.createElement)("svg",{ref:a,...i,width:r,height:r,stroke:t,strokeWidth:n?24*Number(s)/Number(r):s,className:c("lucide",m),...!o&&!(e=>{for(let a in e)if(a.startsWith("aria-")||"role"===a||"title"===a)return!0})(h)&&{"aria-hidden":"true"},...h},[...u.map(e=>{let[a,t]=e;return(0,l.createElement)(a,t)}),...Array.isArray(o)?o:[o]])}),m=(e,a)=>{let t=(0,l.forwardRef)((t,r)=>{let{className:i,...m}=t;return(0,l.createElement)(n,{ref:r,iconNode:a,className:c("lucide-".concat(s(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()),"lucide-".concat(e),i),...m})});return t.displayName=s(e),t},o=m("search",[["path",{d:"m21 21-4.34-4.34",key:"14j7rj"}],["circle",{cx:"11",cy:"11",r:"8",key:"4ej97u"}]]),u=m("rotate-ccw",[["path",{d:"M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8",key:"1357e3"}],["path",{d:"M3 3v5h5",key:"1xhq8a"}]]),h=[{name:"Gemma3-1B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.298,stderr:.0126},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.2024,stderr:.0111}]},{name:"Gemma3-1B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.2768,stderr:.0031},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc_norm",value:.2768,stderr:.0031},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.2617,stderr:.0074},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.2621,stderr:.0105},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.2792,stderr:.0047},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.2865,stderr:.0058}]},{name:"Gemma3-4B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.6126,stderr:.0134},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.4602,stderr:.0137}]},{name:"Gemma3-4B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.3911,stderr:.0034},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.3863,stderr:.0081},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.3205,stderr:.011},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3767,stderr:.0051},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.4366,stderr:.0063}]},{name:"Gemma3-12B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.8271,stderr:.0104},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.7968,stderr:.0111}]},{name:"Gemma3-27B-IT-QAT-Q4_0 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.859,stderr:.0096},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.8514,stderr:.0098}]},{name:"Gemma3n:E2B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.6907,stderr:.0127},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.602,stderr:.0135}]},{name:"Gemma3n:E4B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.7726,stderr:.0115},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.6763,stderr:.0129}]},{name:"Gemma3n:E4B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.711144806671721,stderr:.012484219800126671},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.6391205458680819,stderr:.013228626753925141},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.34305555555555556,stderr:.003308396763985619},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.32457142857142857,stderr:.007900501536176255},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.31310266591038005,stderr:.011011794095963508},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3344893164783533,stderr:.004949253583852297},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.3756294058408862,stderr:.0061661873750182145}]},{name:"GPT-OSS-20B-MXFP4 (llama.cpp)",description:"HF: bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental, ThinkLevel: medium - 運行約 1 小時半",category:"gpt-oss",hardware:"RTX4090",framework:"llama.cpp",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.4344,stderr:.0137},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.3397,stderr:.013}]},{name:"GPT-OSS-20B-MXFP4 (ollama)",description:"ollama 官方儲存庫, MXFP4, ThinkLevel: medium - 運行約 1 小時",category:"gpt-oss",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.395,stderr:.0135},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.1759,stderr:.0105}]},{name:"Llama-3-Taiwan-8B-Instruct-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"llama3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.7376800606520091,stderr:.012116912419925704},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.6982562547384382,stderr:.012643544762873356},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.6487103174603175,stderr:.0032403073893666604},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.618,stderr:.0077206112668541365},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.6222348269994328,stderr:.011351445998177665},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.6275869784092181,stderr:.004954629551174923},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.706277274253105,stderr:.005753468936737745}]},{name:"Llama-3.1-TAIDE-LX-8B-Chat-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"llama3.1",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.1941,stderr:.0109},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:0,stderr:0},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.3866,stderr:.0034},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.354,stderr:.008},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.354,stderr:.0114},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3978,stderr:.0051},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.4047,stderr:.0063}]},{name:"Gemma3-1B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.3153904473085671,stderr:.012799353675801836},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.2357846853677028,stderr:.0116925156506668},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.27232142857142855,stderr:.003129046095819468},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.2702857142857143,stderr:.00749314569360198},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.25921724333522406,stderr:.010445922718571544},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.26893388522206063,stderr:.004680603921898167},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.2824773413897281,stderr:.005818403123869934}]}],d=e=>(100*e).toFixed(2)+"%",v=m("trophy",[["path",{d:"M10 14.66v1.626a2 2 0 0 1-.976 1.696A5 5 0 0 0 7 21.978",key:"1n3hpd"}],["path",{d:"M14 14.66v1.626a2 2 0 0 0 .976 1.696A5 5 0 0 1 17 21.978",key:"rfe1zi"}],["path",{d:"M18 9h1.5a1 1 0 0 0 0-5H18",key:"7xy6bh"}],["path",{d:"M4 22h16",key:"57wxv0"}],["path",{d:"M6 9a6 6 0 0 0 12 0V3a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z",key:"1mhfuq"}],["path",{d:"M6 9H4.5a1 1 0 0 1 0-5H6",key:"tex48p"}]]),g=m("medal",[["path",{d:"M7.21 15 2.66 7.14a2 2 0 0 1 .13-2.2L4.4 2.8A2 2 0 0 1 6 2h12a2 2 0 0 1 1.6.8l1.6 2.14a2 2 0 0 1 .14 2.2L16.79 15",key:"143lza"}],["path",{d:"M11 12 5.12 2.2",key:"qhuxz6"}],["path",{d:"m13 12 5.88-9.8",key:"hbye0f"}],["path",{d:"M8 7h8",key:"i86dvs"}],["circle",{cx:"12",cy:"17",r:"5",key:"qbz8iq"}],["path",{d:"M12 18v-2h-.5",key:"fawc4q"}]]),x=m("award",[["path",{d:"m15.477 12.89 1.515 8.526a.5.5 0 0 1-.81.47l-3.58-2.687a1 1 0 0 0-1.197 0l-3.586 2.686a.5.5 0 0 1-.81-.469l1.514-8.526",key:"1yiouv"}],["circle",{cx:"12",cy:"8",r:"6",key:"1vp47v"}]]),k=(0,l.memo)(e=>{let{rank:a}=e;return(0,r.jsx)("div",{className:"rank-cell ".concat(a<=3?"top-rank":""),children:(0,r.jsxs)("div",{className:"flex items-center justify-center gap-1",children:[1===a?(0,r.jsx)(v,{className:"w-4 h-4 text-yellow-500"}):2===a?(0,r.jsx)(g,{className:"w-4 h-4 text-gray-400"}):3===a?(0,r.jsx)(x,{className:"w-4 h-4 text-amber-600"}):null,(0,r.jsx)("span",{children:a})]})})});k.displayName="RankCell";let p=(0,l.memo)(e=>{let{name:a,description:t,category:l,hardware:s}=e;return(0,r.jsxs)("div",{className:"model-info",children:[(0,r.jsx)("div",{className:"model-icon",style:{backgroundColor:{gemma3:"#3b82f6",gemma3n:"#8b5cf6","gpt-oss":"#10b981",llama3:"#f59e0b","llama3.1":"#ef4444"}[l]||"#6b7280"},children:a.charAt(0).toUpperCase()}),(0,r.jsxs)("div",{children:[(0,r.jsx)("div",{className:"model-name",children:a}),(0,r.jsx)("div",{className:"model-description",children:t})]})]})});p.displayName="ModelInfo";let f=(0,l.memo)(e=>{let{score:a}=e;return(0,r.jsx)("div",{className:"score-cell ".concat((e=>null==e?"score-na":e>=.7?"score-high":e>=.4?"score-medium":"score-low")(a)),children:null!=a?d(a):"—"})});f.displayName="ScoreCell";let j=(0,l.memo)(e=>{let{category:a}=e;return(0,r.jsx)("span",{className:"category-badge ".concat({gemma3:"badge-gemma3",gemma3n:"badge-gemma3n","gpt-oss":"badge-gpt-oss",llama3:"badge-llama3","llama3.1":"badge-llama31"}[a]||"badge-gemma3"),children:a})});function S(){let{data:e,stats:a,benchmark:t,category:s,framework:c,searchTerm:i,setBenchmark:n,setCategory:m,setFramework:v,setSearchTerm:g,resetFilters:x,isEmpty:S,hasFilters:b}=(()=>{let[e,a]=(0,l.useState)("gsm8k-flex"),[t,r]=(0,l.useState)("all"),[s,c]=(0,l.useState)("all"),[i,n]=(0,l.useState)(""),m=(0,l.useMemo)(()=>h.map(a=>{var t,r,l,s,c,i,n;let m=a.scores.find(e=>"gsm8k"===e.task&&"flexible-extract"===e.filter),o=a.scores.find(e=>"gsm8k"===e.task&&"strict-match"===e.filter),u=a.scores.find(e=>"tmmluplus"===e.task),h=a.scores.find(e=>"tmmluplus_STEM"===e.task),d=a.scores.find(e=>"tmmluplus_humanities"===e.task),v=a.scores.find(e=>"tmmluplus_other"===e.task),g=a.scores.find(e=>"tmmluplus_social_sciences"===e.task),x=null!=(t=null==m?void 0:m.value)?t:null,k=null!=(r=null==o?void 0:o.value)?r:null,p=null!=(l=null==u?void 0:u.value)?l:null,f=null!=(s=null==h?void 0:h.value)?s:null,j=null!=(c=null==d?void 0:d.value)?c:null,S=null!=(i=null==v?void 0:v.value)?i:null,b=null!=(n=null==g?void 0:g.value)?n:null,w=a.framework||"unknown",_=null;switch(e){case"gsm8k-flex":_=x;break;case"gsm8k-strict":_=k;break;case"tmmluplus":_=p;break;case"tmmluplus_STEM":_=f;break;case"tmmluplus_humanities":_=j;break;case"tmmluplus_other":_=S;break;case"tmmluplus_social_sciences":_=b}return{rank:0,name:a.name,description:a.description,category:a.category,score:_,allScores:{gsm8kFlex:x,gsm8kStrict:k,tmmluplus:p,tmmluplusSTEM:f,tmmluplusHumanities:j,tmmluplusOther:S,tmmluplus_social_sciences:b},hardware:a.hardware,framework:w}}).sort((e,a)=>null===e.score&&null!==a.score?1:null!==e.score&&null===a.score?-1:null===e.score&&null===a.score?0:a.score-e.score).map((e,a)=>({...e,rank:a+1})),[e]),o=(0,l.useMemo)(()=>m.filter(e=>{let a=e.name.toLowerCase().includes(i.toLowerCase()),r="all"===t||e.category===t,l="all"===s||e.framework===s;return a&&r&&l}),[m,i,t,s]),u=(0,l.useMemo)(()=>{let e=h.length,a=m.filter(e=>null!==e.score).length,t=m.filter(e=>null!==e.score).reduce((e,a)=>e+(a.score||0),0)/a,r=Math.max(...m.map(e=>e.score||0));return{totalModels:e,modelsWithCurrentScore:a,avgScore:isNaN(t)?0:t,topScore:isFinite(r)?r:0}},[m]),d=(0,l.useCallback)(e=>{a(e)},[]),v=(0,l.useCallback)(e=>{r(e)},[]),g=(0,l.useCallback)(e=>{n(e)},[]),x=(0,l.useCallback)(()=>{r("all"),c("all"),n("")},[]);return{data:o,stats:u,benchmark:e,category:t,framework:s,searchTerm:i,setBenchmark:d,setCategory:v,setFramework:(0,l.useCallback)(e=>{c(e)},[]),setSearchTerm:g,resetFilters:x,isEmpty:0===o.length,hasFilters:"all"!==t||"all"!==s||""!==i}})();return(0,r.jsxs)("div",{className:"arena-container",children:[(0,r.jsxs)("header",{className:"arena-header",children:[(0,r.jsx)("h1",{className:"arena-title",children:"LLM Benchmark Arena"}),(0,r.jsx)("p",{className:"arena-subtitle",children:"View rankings across various LLMs on their mathematical reasoning and Traditional Chinese understanding capabilities."}),(0,r.jsxs)("div",{className:"arena-stats",children:[(0,r.jsx)("span",{children:"Last Updated: Aug 16, 2025"}),(0,r.jsxs)("span",{children:["Total Models: ",a.totalModels]}),(0,r.jsxs)("span",{children:["Active Models: ",a.modelsWithCurrentScore]}),(0,r.jsxs)("span",{children:["Top Score: ",d(a.topScore)]}),(0,r.jsxs)("span",{children:["Average: ",d(a.avgScore)]})]})]}),(0,r.jsx)("div",{className:"arena-controls",children:(0,r.jsxs)("div",{className:"control-group",children:[(0,r.jsx)("div",{className:"select-wrapper",children:(0,r.jsx)("select",{value:t,onChange:e=>n(e.target.value),className:"arena-select",children:[{value:"gsm8k-flex",label:"GSM8K (Flexible)",emoji:"\uD83D\uDCCA"},{value:"gsm8k-strict",label:"GSM8K (Strict)",emoji:"\uD83D\uDCCA"},{value:"tmmluplus",label:"TMMLU+",emoji:"\uD83C\uDDF9\uD83C\uDDFC"},{value:"tmmluplus_STEM",label:"TMMLU+ STEM",emoji:"\uD83D\uDD2C"},{value:"tmmluplus_humanities",label:"TMMLU+ Humanities",emoji:"\uD83D\uDCDA"},{value:"tmmluplus_other",label:"TMMLU+ Other",emoji:"\uD83D\uDCDD"},{value:"tmmluplus_social_sciences",label:"TMMLU+ Social Sciences",emoji:"\uD83C\uDFDB️"}].map(e=>(0,r.jsxs)("option",{value:e.value,children:[e.emoji," ",e.label]},e.value))})}),(0,r.jsx)("div",{className:"select-wrapper",children:(0,r.jsx)("select",{value:s,onChange:e=>m(e.target.value),className:"arena-select",children:[{value:"all",label:"All Categories"},{value:"gemma3",label:"Gemma3"},{value:"gemma3n",label:"Gemma3n"},{value:"gpt-oss",label:"GPT-OSS"},{value:"llama3",label:"Llama3"},{value:"llama3.1",label:"Llama3.1"}].map(e=>(0,r.jsx)("option",{value:e.value,children:e.label},e.value))})}),(0,r.jsx)("div",{className:"select-wrapper",children:(0,r.jsx)("select",{value:c,onChange:e=>v(e.target.value),className:"arena-select",children:[{value:"all",label:"All Frameworks",emoji:"\uD83D\uDD27"},{value:"huggingface",label:"Hugging Face",emoji:"\uD83E\uDD17"},{value:"ollama",label:"Ollama",emoji:"\uD83E\uDD99"},{value:"llama.cpp",label:"llama.cpp",emoji:"⚡"}].map(e=>(0,r.jsxs)("option",{value:e.value,children:[e.emoji," ",e.label]},e.value))})}),(0,r.jsxs)("div",{className:"search-container",children:[(0,r.jsx)(o,{className:"search-icon"}),(0,r.jsx)("input",{type:"text",placeholder:"Search by model name...",value:i,onChange:e=>g(e.target.value),className:"search-input"})]}),b&&(0,r.jsxs)("button",{onClick:x,className:"reset-button",title:"Reset filters",children:[(0,r.jsx)(u,{className:"w-4 h-4"}),"Reset"]})]})}),b&&(0,r.jsxs)("div",{className:"px-4 py-2 text-sm text-slate-400",children:["Showing ",e.length," of ",a.totalModels," models",i&&' matching "'.concat(i,'"'),"all"!==s&&" in ".concat(s)]}),(0,r.jsx)("div",{className:"arena-table-container",children:S?(0,r.jsxs)("div",{className:"p-8 text-center",children:[(0,r.jsx)("div",{className:"text-slate-500 mb-2",children:"No models found"}),(0,r.jsx)("div",{className:"text-sm text-slate-400",children:"Try adjusting your search criteria or filters"})]}):(0,r.jsxs)("table",{className:"arena-table",children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Rank"}),(0,r.jsx)("th",{children:"Model"}),(0,r.jsx)("th",{children:"Score"}),(0,r.jsx)("th",{children:"Category"}),(0,r.jsx)("th",{children:"Hardware"})]})}),(0,r.jsx)("tbody",{children:e.map(e=>(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:(0,r.jsx)(k,{rank:e.rank})}),(0,r.jsx)("td",{children:(0,r.jsx)(p,{name:e.name,description:e.description,category:e.category,hardware:e.hardware})}),(0,r.jsx)("td",{children:(0,r.jsx)(f,{score:e.score})}),(0,r.jsx)("td",{children:(0,r.jsx)(j,{category:e.category})}),(0,r.jsx)("td",{className:"text-center",children:e.hardware?(0,r.jsx)("span",{className:"text-xs bg-red-500/20 text-red-400 px-2 py-1 rounded",children:e.hardware}):(0,r.jsx)("span",{className:"text-gray-500",children:"RTX4090"})})]},e.name))})]})}),(0,r.jsx)("div",{className:"mt-8 p-4 bg-slate-800 rounded-lg border border-slate-600",children:(0,r.jsxs)("div",{className:"grid md:grid-cols-2 gap-4 text-sm text-slate-300",children:[(0,r.jsxs)("div",{children:[(0,r.jsx)("h4",{className:"font-semibold text-slate-200 mb-2",children:"Test Environment"}),(0,r.jsxs)("ul",{className:"space-y-1",children:[(0,r.jsx)("li",{children:"• OS: Linux (Ubuntu 22.04)"}),(0,r.jsx)("li",{children:"• Primary GPU: RTX4090 24GB"}),(0,r.jsx)("li",{children:"• Secondary GPU: RTX5090 32GB (marked)"}),(0,r.jsx)("li",{children:"• Framework: EleutherAI/lm-evaluation-harness"})]})]}),(0,r.jsxs)("div",{children:[(0,r.jsx)("h4",{className:"font-semibold text-slate-200 mb-2",children:"Benchmarks"}),(0,r.jsxs)("ul",{className:"space-y-1",children:[(0,r.jsxs)("li",{children:["• ",(0,r.jsx)("strong",{children:"GSM8K:"})," Mathematical reasoning problems"]}),(0,r.jsxs)("li",{children:["• ",(0,r.jsx)("strong",{children:"TMMLU+:"})," Taiwan Traditional Chinese understanding"]}),(0,r.jsxs)("li",{children:["• ",(0,r.jsx)("strong",{children:"Flexible:"})," Lenient answer extraction"]}),(0,r.jsxs)("li",{children:["• ",(0,r.jsx)("strong",{children:"Strict:"})," Exact answer matching"]})]})]})]})})]})}j.displayName="CategoryBadge"}},e=>{e.O(0,[441,964,358],()=>e(e.s=8774)),_N_E=e.O()}]);