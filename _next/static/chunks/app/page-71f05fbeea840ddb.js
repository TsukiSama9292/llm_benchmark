(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{5747:(e,t,a)=>{"use strict";a.d(t,{default:()=>N});var s=a(5155),r=a(2115);let l=e=>{let t=e.replace(/^([A-Z])|[\s-_]+(\w)/g,(e,t,a)=>a?a.toUpperCase():t.toLowerCase());return t.charAt(0).toUpperCase()+t.slice(1)},i=function(){for(var e=arguments.length,t=Array(e),a=0;a<e;a++)t[a]=arguments[a];return t.filter((e,t,a)=>!!e&&""!==e.trim()&&a.indexOf(e)===t).join(" ").trim()};var c={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let n=(0,r.forwardRef)((e,t)=>{let{color:a="currentColor",size:s=24,strokeWidth:l=2,absoluteStrokeWidth:n,className:o="",children:m,iconNode:d,...h}=e;return(0,r.createElement)("svg",{ref:t,...c,width:s,height:s,stroke:a,strokeWidth:n?24*Number(l)/Number(s):l,className:i("lucide",o),...!m&&!(e=>{for(let t in e)if(t.startsWith("aria-")||"role"===t||"title"===t)return!0})(h)&&{"aria-hidden":"true"},...h},[...d.map(e=>{let[t,a]=e;return(0,r.createElement)(t,a)}),...Array.isArray(m)?m:[m]])}),o=(e,t)=>{let a=(0,r.forwardRef)((a,s)=>{let{className:c,...o}=a;return(0,r.createElement)(n,{ref:s,iconNode:t,className:i("lucide-".concat(l(e).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()),"lucide-".concat(e),c),...o})});return a.displayName=l(e),a},m=o("search",[["path",{d:"m21 21-4.34-4.34",key:"14j7rj"}],["circle",{cx:"11",cy:"11",r:"8",key:"4ej97u"}]]),d=o("rotate-ccw",[["path",{d:"M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8",key:"1357e3"}],["path",{d:"M3 3v5h5",key:"1xhq8a"}]]),h=[{name:"Gemma3-1B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.298,stderr:.0126},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.2024,stderr:.0111}]},{name:"Gemma3-1B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.3153904473085671,stderr:.012799353675801836},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.2357846853677028,stderr:.0116925156506668},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.27232142857142855,stderr:.003129046095819468},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.2702857142857143,stderr:.00749314569360198},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.25921724333522406,stderr:.010445922718571544},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.26893388522206063,stderr:.004680603921898167},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.2824773413897281,stderr:.005818403123869934}]},{name:"Gemma3-4B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.6126,stderr:.0134},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.4602,stderr:.0137}]},{name:"Gemma3-4B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.7445034116755117,stderr:.012013462405460067},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.5784685367702805,stderr:.013601824409483263},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.30540674603174606,stderr:.0032293422469302567},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.30228571428571427,stderr:.007764817617012654},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.2665910380034033,stderr:.010543420822525719},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3060745049781855,stderr:.004855723662601807},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.31772406847935547,stderr:.005982160599560892}]},{name:"Gemma3-12B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.8271,stderr:.0104},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.7968,stderr:.0111}]},{name:"Gemma3-27B-IT-QAT-Q4_0 (ollama)",description:"ollama 官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.859,stderr:.0096},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.8514,stderr:.0098}]},{name:"Gemma3n:E2B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.6907,stderr:.0127},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.602,stderr:.0135}]},{name:"Gemma3n:E4B-IT-FP16 (ollama)",description:"ollama 官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.7726,stderr:.0115},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.6763,stderr:.0129}]},{name:"Gemma3n:E4B-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3n",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.711144806671721,stderr:.012484219800126671},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.6391205458680819,stderr:.013228626753925141},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.34305555555555556,stderr:.003308396763985619},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.32457142857142857,stderr:.007900501536176255},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.31310266591038005,stderr:.011011794095963508},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3344893164783533,stderr:.004949253583852297},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.3756294058408862,stderr:.0061661873750182145}]},{name:"GPT-OSS-20B-MXFP4 (llama.cpp)",description:"HF: bartowski/openai_gpt-oss-20b-GGUF-MXFP4-Experimental, ThinkLevel: medium - 運行約 1 小時半",category:"gpt-oss",hardware:"RTX4090",framework:"llama.cpp",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.4344,stderr:.0137},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.3397,stderr:.013}]},{name:"GPT-OSS-20B-MXFP4 (ollama)",description:"ollama 官方儲存庫, MXFP4, ThinkLevel: medium - 運行約 1 小時",category:"gpt-oss",hardware:"RTX4090",framework:"ollama",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.395,stderr:.0135},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.1759,stderr:.0105}]},{name:"Llama-3-Taiwan-8B-Instruct-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"llama3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.7376800606520091,stderr:.012116912419925704},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.6982562547384382,stderr:.012643544762873356},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.6487103174603175,stderr:.0032403073893666604},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.618,stderr:.0077206112668541365},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.6222348269994328,stderr:.011351445998177665},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.6275869784092181,stderr:.004954629551174923},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.706277274253105,stderr:.005753468936737745}]},{name:"Llama-3.1-TAIDE-LX-8B-Chat-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"llama3.1",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.1941,stderr:.0109},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:0,stderr:0},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.3866,stderr:.0034},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.354,stderr:.008},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.354,stderr:.0114},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3978,stderr:.0051},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.4047,stderr:.0063}]},{name:"Gemma3-1B-PT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.02350265352539803,stderr:.004172883669643984},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.017437452615617893,stderr:.0036054868679982572},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.2536706349206349,stderr:.003067223514675123},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.248,stderr:.0073066838168183695},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.24787294384571754,stderr:.010292872668744832},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.25383152477905807,stderr:.004606655490553888},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.25847599865726756,stderr:.0056783771037919985}]},{name:"Gemma3-4B-PT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.37452615617892343,stderr:.013331774158491384},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.37225170583775585,stderr:.013315375362565038},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.3748015873015873,stderr:.003369815844399701},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.36942857142857144,stderr:.008079038279183059},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.32671582529778787,stderr:.011026563399222476},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.3660364694037364,stderr:.005051945632445054},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.405337361530715,stderr:.006276382491520932}]},{name:"Gemma3-270M-IT-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.018953752843062926,stderr:.003756078341031473},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:0,stderr:0},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.25124007936507936,stderr:.003058923433973526},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.25485714285714284,stderr:.007379903664395711},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.25070901871809415,stderr:.010336250088913018},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.24924488197784986,stderr:.004580979422460125},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.25226586102719034,stderr:.0056341894020538255}]},{name:"Gemma3-270M-BF16 (huggingface)",description:"HuggingFace 模型官方儲存庫",category:"gemma3",hardware:"RTX4090",framework:"huggingface",scores:[{task:"gsm8k",version:3,filter:"flexible-extract",nShot:5,metric:"exact_match",value:.01819560272934041,stderr:.0036816118940738705},{task:"gsm8k",version:3,filter:"strict-match",nShot:5,metric:"exact_match",value:.012130401819560273,stderr:.0030152942428909395},{task:"tmmluplus",version:2,filter:"none",nShot:0,metric:"acc",value:.2556051587301587,stderr:.00307431083242714},{task:"tmmluplus_STEM",version:2,filter:"none",nShot:0,metric:"acc",value:.2557142857142857,stderr:.007383170100357892},{task:"tmmluplus_humanities",version:2,filter:"none",nShot:0,metric:"acc",value:.2614861032331254,stderr:.010480863869935854},{task:"tmmluplus_other",version:2,filter:"none",nShot:0,metric:"acc",value:.2596487302830294,stderr:.004639504861875137},{task:"tmmluplus_social_sciences",version:2,filter:"none",nShot:0,metric:"acc",value:.24773413897280966,stderr:.005597285695169396}]}],u=e=>(100*e).toFixed(2)+"%",x={environment:{os:"Ubuntu 22.04",framework:"lm-evaluation-harness",primaryGpu:"RTX4090 24GB",secondaryGpu:"RTX5090 32GB"},benchmarks:{gsm8k:"Expert-written math benchmark covering multi-step elementary-level word problems (English), ~8,500 questions (7.5K train, 1K test).",tmmluplus:"Traditional Chinese multiple-choice cognitive benchmark, 66 domains (elementary to professional), ~22,690 questions, 6x larger and more balanced than original TMMLU."},license:{type:"MIT",copyright:"Copyright",year:"2025",holder:"Xuan-You Lin"}},v=o("trophy",[["path",{d:"M10 14.66v1.626a2 2 0 0 1-.976 1.696A5 5 0 0 0 7 21.978",key:"1n3hpd"}],["path",{d:"M14 14.66v1.626a2 2 0 0 0 .976 1.696A5 5 0 0 1 17 21.978",key:"rfe1zi"}],["path",{d:"M18 9h1.5a1 1 0 0 0 0-5H18",key:"7xy6bh"}],["path",{d:"M4 22h16",key:"57wxv0"}],["path",{d:"M6 9a6 6 0 0 0 12 0V3a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z",key:"1mhfuq"}],["path",{d:"M6 9H4.5a1 1 0 0 1 0-5H6",key:"tex48p"}]]),g=o("medal",[["path",{d:"M7.21 15 2.66 7.14a2 2 0 0 1 .13-2.2L4.4 2.8A2 2 0 0 1 6 2h12a2 2 0 0 1 1.6.8l1.6 2.14a2 2 0 0 1 .14 2.2L16.79 15",key:"143lza"}],["path",{d:"M11 12 5.12 2.2",key:"qhuxz6"}],["path",{d:"m13 12 5.88-9.8",key:"hbye0f"}],["path",{d:"M8 7h8",key:"i86dvs"}],["circle",{cx:"12",cy:"17",r:"5",key:"qbz8iq"}],["path",{d:"M12 18v-2h-.5",key:"fawc4q"}]]),p=o("award",[["path",{d:"m15.477 12.89 1.515 8.526a.5.5 0 0 1-.81.47l-3.58-2.687a1 1 0 0 0-1.197 0l-3.586 2.686a.5.5 0 0 1-.81-.469l1.514-8.526",key:"1yiouv"}],["circle",{cx:"12",cy:"8",r:"6",key:"1vp47v"}]]),f=(0,r.memo)(e=>{let{rank:t}=e;return(0,s.jsx)("div",{className:"rank-cell ".concat(t<=3?"top-rank":""),children:(0,s.jsxs)("div",{className:"flex items-center justify-center gap-1",children:[1===t?(0,s.jsx)(v,{className:"w-4 h-4 text-yellow-500"}):2===t?(0,s.jsx)(g,{className:"w-4 h-4 text-gray-400"}):3===t?(0,s.jsx)(p,{className:"w-4 h-4 text-amber-600"}):null,(0,s.jsx)("span",{children:t})]})})});f.displayName="RankCell";let k=(0,r.memo)(e=>{let{name:t,description:a,category:r,hardware:l}=e;return(0,s.jsxs)("div",{className:"model-info",children:[(0,s.jsx)("div",{className:"model-icon",style:{backgroundColor:{gemma3:"#3b82f6",gemma3n:"#8b5cf6","gpt-oss":"#10b981",llama3:"#f59e0b","llama3.1":"#ef4444"}[r]||"#6b7280"},children:t.charAt(0).toUpperCase()}),(0,s.jsxs)("div",{children:[(0,s.jsx)("div",{className:"model-name",children:t}),(0,s.jsx)("div",{className:"model-description",children:a})]})]})});k.displayName="ModelInfo";let j=(0,r.memo)(e=>{let{score:t}=e;return(0,s.jsx)("div",{className:"score-cell ".concat((e=>null==e?"score-na":e>=.7?"score-high":e>=.4?"score-medium":"score-low")(t)),children:null!=t?u(t):"—"})});j.displayName="ScoreCell";let b=(0,r.memo)(e=>{let{category:t}=e;return(0,s.jsx)("span",{className:"category-badge ".concat({gemma3:"badge-gemma3",gemma3n:"badge-gemma3n","gpt-oss":"badge-gpt-oss",llama3:"badge-llama3","llama3.1":"badge-llama31"}[t]||"badge-gemma3"),children:t})});function S(e){let{icon:t,title:a,description:r,children:l,gradient:i}=e;return(0,s.jsxs)("div",{className:"relative group",children:[(0,s.jsx)("div",{className:"absolute inset-0 bg-gradient-to-br from-blue-500/20 via-cyan-500/10 to-blue-600/20 rounded-xl blur-xl opacity-30 group-hover:opacity-50 transition-opacity duration-300"}),(0,s.jsxs)("div",{className:"relative bg-slate-800/90 backdrop-blur-sm border border-slate-600/50 rounded-xl shadow-xl overflow-hidden hover:shadow-2xl transition-all duration-300 group-hover:border-blue-500/30",children:[(0,s.jsx)("div",{className:"absolute inset-0 bg-gradient-to-br ".concat(i," opacity-5")}),(0,s.jsxs)("div",{className:"relative p-6",children:[(0,s.jsxs)("div",{className:"flex items-center mb-6",children:[(0,s.jsx)("div",{className:"w-12 h-12 bg-gradient-to-br ".concat(i," rounded-xl flex items-center justify-center mr-4 flex-shrink-0 shadow-lg group-hover:scale-105 transition-transform duration-300"),children:t?(0,s.jsx)("span",{className:"text-white text-xl",children:t}):(0,s.jsx)("div",{className:"w-6 h-6 bg-white/20 rounded-md"})}),(0,s.jsxs)("div",{className:"min-w-0 flex-1",children:[(0,s.jsx)("h3",{className:"text-xl font-bold text-slate-100 group-hover:text-white transition-colors duration-300",children:a}),(0,s.jsx)("p",{className:"text-slate-400 text-sm mt-1",children:r})]})]}),(0,s.jsx)("div",{className:"text-left",children:l})]}),(0,s.jsx)("div",{className:"h-1 bg-gradient-to-r ".concat(i," opacity-60")})]})]})}function y(e){let{environment:t}=e;return t.os,t.framework,t.primaryGpu,t.secondaryGpu,(0,s.jsx)(S,{icon:"",title:"",description:"",gradient:"from-blue-500 to-indigo-600",children:(0,s.jsxs)("div",{children:[(0,s.jsx)("h2",{className:"text-base font-bold text-slate-100",children:"Test Environment"}),(0,s.jsx)("div",{className:"text-slate-400 text-xs",children:"Hardware and software specifications"}),(0,s.jsxs)("div",{className:"space-y-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-blue-300",children:"OS"}),(0,s.jsx)("div",{className:"text-slate-100 font-mono text-xs",children:t.os})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-blue-300",children:"FW"}),(0,s.jsx)("div",{className:"text-slate-100 font-mono text-xs",children:t.framework})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-blue-300",children:"GPU1"}),(0,s.jsx)("div",{className:"text-slate-100 font-mono text-xs",children:t.primaryGpu})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-blue-300",children:"GPU2"}),(0,s.jsx)("div",{className:"text-slate-100 font-mono text-xs",children:t.secondaryGpu})]})]})]})})}function _(e){let{benchmarks:t}=e;return t.gsm8k,t.tmmluplus,(0,s.jsx)(S,{icon:"",title:"",description:"",gradient:"from-purple-500 to-pink-600",children:(0,s.jsxs)("div",{children:[(0,s.jsx)("h2",{className:"text-base font-bold text-slate-100",children:"Evaluation Benchmarks"}),(0,s.jsx)("div",{className:"text-slate-400 text-xs",children:"Comprehensive assessment methodology"}),(0,s.jsxs)("div",{className:"space-y-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-emerald-300",children:"MATH"}),(0,s.jsxs)("div",{className:"font-bold text-slate-200 text-xs",children:["GSM8K: ",t.gsm8k]})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-purple-300",children:"LANG"}),(0,s.jsxs)("div",{className:"font-bold text-slate-200 text-xs",children:["TMML+: ",t.tmmluplus]})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-amber-300",children:"METHOD"}),(0,s.jsx)("div",{className:"font-bold text-slate-200 text-xs",children:"Flexible: Lenient answer extraction from output"}),(0,s.jsx)("div",{className:"font-bold text-slate-200 text-xs",children:"Strict: Requires output to match specified format"})]})]})]})})}function w(e){let{license:t}=e;return(0,s.jsx)(S,{icon:"",title:"",description:"",gradient:"from-green-500 to-emerald-600",children:(0,s.jsxs)("div",{children:[(0,s.jsx)("h2",{className:"text-base font-bold text-slate-100",children:"License Information"}),(0,s.jsx)("div",{className:"text-slate-400 text-xs",children:"Open source licensing details"}),(0,s.jsxs)("div",{className:"space-y-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-green-300",children:"LICENSE"}),(0,s.jsx)("div",{className:"text-slate-100 font-mono text-xs",children:t.type})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-green-300",children:"COPYRIGHT"}),(0,s.jsxs)("div",{className:"text-slate-300 text-xs",children:[t.copyright," ",t.year," ",t.holder]})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"text-sm font-bold text-green-300",children:"TERMS"}),(0,s.jsx)("div",{className:"text-slate-400 text-xs leading-relaxed",children:"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files."})]})]})]})})}function N(){let{data:e,stats:t,benchmark:a,category:l,framework:i,searchTerm:c,setBenchmark:n,setCategory:o,setFramework:v,setSearchTerm:g,resetFilters:p,isEmpty:S,hasFilters:N}=(()=>{let[e,t]=(0,r.useState)("gsm8k-flex"),[a,s]=(0,r.useState)("all"),[l,i]=(0,r.useState)("all"),[c,n]=(0,r.useState)(""),o=(0,r.useMemo)(()=>h.map(t=>{var a,s,r,l,i,c,n;let o=t.scores.find(e=>"gsm8k"===e.task&&"flexible-extract"===e.filter),m=t.scores.find(e=>"gsm8k"===e.task&&"strict-match"===e.filter),d=t.scores.find(e=>"tmmluplus"===e.task),h=t.scores.find(e=>"tmmluplus_STEM"===e.task),u=t.scores.find(e=>"tmmluplus_humanities"===e.task),x=t.scores.find(e=>"tmmluplus_other"===e.task),v=t.scores.find(e=>"tmmluplus_social_sciences"===e.task),g=null!=(a=null==o?void 0:o.value)?a:null,p=null!=(s=null==m?void 0:m.value)?s:null,f=null!=(r=null==d?void 0:d.value)?r:null,k=null!=(l=null==h?void 0:h.value)?l:null,j=null!=(i=null==u?void 0:u.value)?i:null,b=null!=(c=null==x?void 0:x.value)?c:null,S=null!=(n=null==v?void 0:v.value)?n:null,y=t.framework||"unknown",_=null;switch(e){case"gsm8k-flex":_=g;break;case"gsm8k-strict":_=p;break;case"tmmluplus":_=f;break;case"tmmluplus_STEM":_=k;break;case"tmmluplus_humanities":_=j;break;case"tmmluplus_other":_=b;break;case"tmmluplus_social_sciences":_=S}return{rank:0,name:t.name,description:t.description,category:t.category,score:_,allScores:{gsm8kFlex:g,gsm8kStrict:p,tmmluplus:f,tmmluplusSTEM:k,tmmluplusHumanities:j,tmmluplusOther:b,tmmluplus_social_sciences:S},hardware:t.hardware,framework:y}}).sort((e,t)=>null===e.score&&null!==t.score?1:null!==e.score&&null===t.score?-1:null===e.score&&null===t.score?0:t.score-e.score).map((e,t)=>({...e,rank:t+1})),[e]),m=(0,r.useMemo)(()=>o.filter(e=>{let t=e.name.toLowerCase().includes(c.toLowerCase()),s="all"===a||e.category===a,r="all"===l||e.framework===l;return t&&s&&r}),[o,c,a,l]),d=(0,r.useMemo)(()=>{let e=h.length,t=o.filter(e=>null!==e.score).length,a=o.filter(e=>null!==e.score).reduce((e,t)=>e+(t.score||0),0)/t,s=Math.max(...o.map(e=>e.score||0));return{totalModels:e,modelsWithCurrentScore:t,avgScore:isNaN(a)?0:a,topScore:isFinite(s)?s:0}},[o]),u=(0,r.useCallback)(e=>{t(e)},[]),x=(0,r.useCallback)(e=>{s(e)},[]),v=(0,r.useCallback)(e=>{n(e)},[]),g=(0,r.useCallback)(()=>{s("all"),i("all"),n("")},[]);return{data:m,stats:d,benchmark:e,category:a,framework:l,searchTerm:c,setBenchmark:u,setCategory:x,setFramework:(0,r.useCallback)(e=>{i(e)},[]),setSearchTerm:v,resetFilters:g,isEmpty:0===m.length,hasFilters:"all"!==a||"all"!==l||""!==c}})();return(0,s.jsxs)("div",{className:"arena-container",children:[(0,s.jsxs)("header",{className:"arena-header",children:[(0,s.jsx)("h1",{className:"arena-title",children:"LLM Benchmark Arena"}),(0,s.jsx)("p",{className:"arena-subtitle",children:"View rankings across various LLMs on their mathematical reasoning and Traditional Chinese understanding capabilities."}),(0,s.jsxs)("div",{className:"arena-stats",children:[(0,s.jsx)("span",{children:"Last Updated: Aug 16, 2025"}),(0,s.jsxs)("span",{children:["Total Models: ",t.totalModels]}),(0,s.jsxs)("span",{children:["Active Models: ",t.modelsWithCurrentScore]}),(0,s.jsxs)("span",{children:["Top Score: ",u(t.topScore)]}),(0,s.jsxs)("span",{children:["Average: ",u(t.avgScore)]})]})]}),(0,s.jsx)("div",{className:"arena-controls",children:(0,s.jsxs)("div",{className:"control-group",children:[(0,s.jsx)("div",{className:"select-wrapper",children:(0,s.jsx)("select",{value:a,onChange:e=>n(e.target.value),className:"arena-select",children:[{value:"gsm8k-flex",label:"GSM8K (Flexible)",emoji:"\uD83D\uDCCA"},{value:"gsm8k-strict",label:"GSM8K (Strict)",emoji:"\uD83D\uDCCA"},{value:"tmmluplus",label:"TMMLU+",emoji:"\uD83C\uDDF9\uD83C\uDDFC"},{value:"tmmluplus_STEM",label:"TMMLU+ STEM",emoji:"\uD83D\uDD2C"},{value:"tmmluplus_humanities",label:"TMMLU+ Humanities",emoji:"\uD83D\uDCDA"},{value:"tmmluplus_other",label:"TMMLU+ Other",emoji:"\uD83D\uDCDD"},{value:"tmmluplus_social_sciences",label:"TMMLU+ Social Sciences",emoji:"\uD83C\uDFDB️"}].map(e=>(0,s.jsx)("option",{value:e.value,children:e.label},e.value))})}),(0,s.jsx)("div",{className:"select-wrapper",children:(0,s.jsx)("select",{value:l,onChange:e=>o(e.target.value),className:"arena-select",children:[{value:"all",label:"All Categories"},{value:"gemma3",label:"Gemma3"},{value:"gemma3n",label:"Gemma3n"},{value:"gpt-oss",label:"GPT-OSS"},{value:"llama3",label:"Llama3"},{value:"llama3.1",label:"Llama3.1"}].map(e=>(0,s.jsx)("option",{value:e.value,children:e.label},e.value))})}),(0,s.jsx)("div",{className:"select-wrapper",children:(0,s.jsx)("select",{value:i,onChange:e=>v(e.target.value),className:"arena-select",children:[{value:"all",label:"All Frameworks",emoji:"\uD83D\uDD27"},{value:"huggingface",label:"Hugging Face",emoji:"\uD83E\uDD17"},{value:"ollama",label:"Ollama",emoji:"\uD83E\uDD99"},{value:"llama.cpp",label:"llama.cpp",emoji:"⚡"}].map(e=>(0,s.jsxs)("option",{value:e.value,children:[e.emoji," ",e.label]},e.value))})}),(0,s.jsxs)("div",{className:"search-container",children:[(0,s.jsx)(m,{className:"search-icon"}),(0,s.jsx)("input",{type:"text",placeholder:"Search by model name...",value:c,onChange:e=>g(e.target.value),className:"search-input"})]}),N&&(0,s.jsxs)("button",{onClick:p,className:"reset-button",title:"Reset filters",children:[(0,s.jsx)(d,{className:"w-4 h-4"}),"Reset"]})]})}),N&&(0,s.jsxs)("div",{className:"px-4 py-2 text-sm text-slate-400",children:["Showing ",e.length," of ",t.totalModels," models",c&&' matching "'.concat(c,'"'),"all"!==l&&" in ".concat(l)]}),(0,s.jsx)("div",{className:"arena-table-container",children:S?(0,s.jsxs)("div",{className:"p-8 text-center",children:[(0,s.jsx)("div",{className:"text-slate-500 mb-2",children:"No models found"}),(0,s.jsx)("div",{className:"text-sm text-slate-400",children:"Try adjusting your search criteria or filters"})]}):(0,s.jsxs)("table",{className:"arena-table",children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Rank"}),(0,s.jsx)("th",{children:"Model"}),(0,s.jsx)("th",{children:"Score"}),(0,s.jsx)("th",{children:"Category"}),(0,s.jsx)("th",{children:"Hardware"})]})}),(0,s.jsx)("tbody",{children:e.map(e=>(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:(0,s.jsx)(f,{rank:e.rank})}),(0,s.jsx)("td",{children:(0,s.jsx)(k,{name:e.name,description:e.description,category:e.category,hardware:e.hardware})}),(0,s.jsx)("td",{children:(0,s.jsx)(j,{score:e.score})}),(0,s.jsx)("td",{children:(0,s.jsx)(b,{category:e.category})}),(0,s.jsx)("td",{className:"text-center",children:e.hardware?(0,s.jsx)("span",{className:"text-xs bg-red-500/20 text-red-400 px-2 py-1 rounded",children:e.hardware}):(0,s.jsx)("span",{className:"text-gray-500",children:"RTX4090"})})]},e.name))})]})}),(0,s.jsxs)("div",{className:"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6 mt-8",children:[(0,s.jsx)(y,{environment:x.environment}),(0,s.jsx)(_,{benchmarks:x.benchmarks}),(0,s.jsx)(w,{license:x.license})]})]})}b.displayName="CategoryBadge"},8774:(e,t,a)=>{Promise.resolve().then(a.bind(a,5747))}},e=>{e.O(0,[441,964,358],()=>e(e.s=8774)),_N_E=e.O()}]);